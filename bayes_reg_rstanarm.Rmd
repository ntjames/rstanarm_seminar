---
title: "Introduction to Bayesian Regression Modeling in R using rstanarm"
author: "Nathan T. James"
date: "<small>`r Sys.Date()`</small>"
output:
  html_document:
    toc: no
    toc_depth: 3
    number_sections: false
    toc_float:
      collapsed: false
    code_folding: hide
    theme: paper
code_folding: hide
description: "Intro to Bayes Regression with rstanarm"
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir=file.path("/home/nathan/Dropbox/njames/school/PhD/misc/stat_computing_seminar/rstanarm_seminar"))

#add path to pdflatex (http://stackoverflow.com/questions/30601171/add-tex-path-to-r-studio-ubuntu)
#Sys.setenv(PATH = paste(Sys.getenv("PATH"), "/usr/local/texlive/2016/bin/x86_64-linux/", sep=":"))
set.seed(938873)
```

## Introduction

The Bayesian paradigm has become increasingly popular, but is still not as widespread as "classical" statistical methods (e.g. maximum likelihood estimation, null hypothesis significance testing, etc.). One reason for this disparity is the somewhat steep learning curve for Bayesian statistical software. The `rstanarm` package aims to address this gap by allowing `R` users to fit common Bayesian regression models using an interface very similar to standard functions R functions such as `lm()` and `glm()`. In this seminar we will provide an introduction to Bayesian inference and demonstrate how to fit several basic models using `rstanarm`.

### Bayesian Inference

Bayesian inference provides a principled method of combining prior information concerning parameters (such as regression coefficients) with observed outcome and covariate data based on Bayes' Theorem: 
\begin{equation}
p(\theta|X, y)=\frac{p(\theta)p(y|X, \theta)}{\int p(\theta)p(y|X, \theta)\,d\theta}
\end{equation}
For regression modeling, $p(y|\theta, X)$ in this equation is the likelihood from the sampling model and $p(\theta)$ represents prior information (or absence of information) about the parameters. The combination of these two components yields the posterior probability $p(\theta|y, X)$ which represents an updated distribution for the parameters conditional on the observed data. Performing inference for regression models in a Bayesian framework has several advantages:

* Ability to answer inferential questions using interpretable posterior probability statements rather than hypothesis tests and p-values which are dominant in the frequentist paradigm (e.g., "Given our model, prior knowledge, and the observed data, there is a 91% probability that the drug is effective")

* Can formally incorporate information from multiple sources including prior information if available

* All inference proceeds directly from the posterior distribution

### Stan, rstan, and rstanarm

[Stan](https://mc-stan.org/) is a general purpose probabilistic programming language for Bayesian statistical inference. It has interfaces for many popular data analysis languages including `Python`, `MATLAB`, and `Stata`. The `R` interface for `Stan` is called `rstan` and `rstanarm` is a front-end to `rstan` that allows regression models to be fit using a standard `R` regression model interface. 

```{r load_lib, message=FALSE}
# install.packages("rstanarm") # may take a while to install
library(rstan)
library(rstanarm)
```


### Steps for Bayesian inference

1. Specify the probability model. The model has two parts, a likelihood for the observed data and a prior distribution. In practice, the form of the likelihood is often based on the outcome type and is often the same as a frequentist analysis. Specifying the prior distribution can be more involved, but `rstanarm` includes default priors that work well in many cases.

2. Draw samples from the posterior distribution. Once the model is specified, we need to get an updated distribution of the parameters conditional on the observed data. Conceptually, this step is simple, but in practice it can be quite complicated. By default, `Stan` uses a method called Markov Chain Monte Carlo (MCMC) to get these samples.

3. Evaluate the model. There are several important checks that are necessary to ensure that there are no problems with the MCMC procedure used to get samples or the posterior distribution.

4. Perform inference. Once we have verified that every looks good with our model and MCMC sampling procedure, we can use the samples to perform inference on any quantity involving the posterior parameter distribution.

## Linear Regression model with continuous outcome

Let's see how to apply these steps for a linear regression model. We'll use the simple `cars` dataset which gives the speed of 50 cars along with the corresponding stopping distances.

```{r plt_cars, fig.align='center'}
library(ggplot2)
qplot(speed, dist, data=cars)
```

1. Specify the probability model. For this example, assume the usual linear regression model where each outcome observation (`dist`) is conditionally normal given the covariate (`speed`) with independent errors, $\text{dist} = \beta_0 + \beta_1\cdot \text{speed} + \varepsilon$, $\varepsilon \sim N(0,\sigma^2)$. This implies the posterior will have 3 parameters, $\beta_0$, $\beta_1$ and $\sigma^2$. We will let `rstanarm` use the default priors for now to complete the model specification.

2. Draw samples from the posterior distribution. We can use the `rstanarm` function `stan_glm()` to draw samples from the posterior using the model above. 

```{r fit_cars, cache=TRUE, message=FALSE}
# this option uses multiple cores if they're available
options(mc.cores = parallel::detectCores()) 
glm_post1 <- stan_glm(dist~speed, data=cars, family=gaussian)
```

3. Evaluate the model. Two common checks for the MCMC sampler are trace plots and $\hat{R}$. We use the function `stan_trace()` to draw the trace plots which show sequential draws from the posterior distribution. Ideally we want the chains in each traceplot to be stable (centered around one value) and well-mixed (all chains are overlapping around the same value).

```{r diag_cars, fig.height=4, fig.width=10, fig.align='center'}
stan_trace(glm_post1, pars=c("(Intercept)","speed","sigma"))
```

$\hat{R}$ uses estimates of variance within and between the chains to monitor convergence. At convergence $\hat{R}=1$.  Summarizing the model fit with `summary()` we see that $\hat{R}$ values are, in fact, 1 for all parameters. The output also gives us information about the model used and estimates of the posterior parameter distributions.

```{r summ_cars}
summary(glm_post1)
```

In addition to checks on the MCMC sampler, we can look at posterior predictive checks.

> The idea behind posterior predictive checking is simple:
> If our model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed. 

The dark blue line shows the observed data while the light blue lines are simulations from the posterior predictive distribution.

```{r ppc_cars, fig.align="center"}
pp_check(glm_post1)

library(bayesplot)
ppc_intervals(
  y = cars$dist,
  yrep = posterior_predict(glm_post1),
  x = cars$speed)
```

There are many other types of checks that can be explored interactively by running the `launch_shinystan()` function on the fit object.

4. Perform inference. We can use the posterior samples to perform inference. Focusing on the `speed` parameter, we can first plot a histogram of the posterior samples.

```{r speed_hist_cars, fig.align="center"}
stan_hist(glm_post1, pars=c("speed"), bins=40)
```

We can also extract these posterior samples to get an estimate of the mean and a 95% credible interval.

```{r inf_cars}
post_samps_speed <- as.data.frame(glm_post1, pars=c("speed"))[,"speed"]
mn_speed <- mean(post_samps_speed)
ci_speed <- quantile(post_samps_speed, probs=c(0.05, 0.95))
```

The speed parameter is the slope of the regression line from the model defined above. The average change in stopping distance associated with a one unit change in speed is `r round(mn_speed,2)`. There is a 95% probability that the change in stopping distance associated with a one unit change in speed is between `r round(ci_speed[1],2)` and `r round(ci_speed[2],2)`. We can also use the posterior to calculate other values of interest, for example the probability that the change in stopping distance for a unit change in speed is greater than 3.5 is `r round(mean(post_samps_speed > 3.5),2)` (calculated using `mean(post_samps_speed > 3.5)`).

Now, let's compare to a frequentist analysis. Under this paradigm, we can fit a linear regression model using `glm(..., family=gaussian)`.

```{r lm_cars}
glm_fit <- glm(dist~speed, data=cars, family=gaussian)
summary(glm_fit)
```

The maximum likelihood estimates for the intercept and slope are `r round(coef(glm_fit)[1],2)` and `r round(coef(glm_fit)[2],2)` which are nearly identical to the Bayesian posterior median values of `r round(fixef(glm_post1)[1],2)` and `r round(fixef(glm_post1)[2],2)`.

We can take a look at default priors used by `rstanarm` with the `prior_summary()` function. The intercept and coefficients both use a normal distribution centered around 0 as a prior, but the distribution for the intercept has higher variance (scale). The error term uses an exponential prior. 

```{r prior_cars}
prior_summary(glm_post1)
```

It can also be helpful to juxtapose intervals from the prior distribution and the posterior distribution to see how the observed data has changed the parameter estimates. 

```{r prior_v_post_cars, cache=TRUE, fig.align="center"}
posterior_vs_prior(glm_post1, group_by_parameter = TRUE)
```

We can use a different prior for the slope by changing the `prior` argument in the call to `stan_glm()`. Let's use a much more informative normal prior for the slope centered around 2 with variance 0.5.

```{r inf_prior_cars, cache=TRUE, message=FALSE}
glm_post2 <- stan_glm(dist~speed, data=cars, family=gaussian, 
                      prior=normal(2, 0.5, autoscale=FALSE))
```

The posterior estimate for the slope is adjusted toward the prior value.

```{r prior_v_post_cars2, cache=TRUE, fig.align="center"}
posterior_vs_prior(glm_post2, pars=c("speed"), group_by_parameter = TRUE)
```

```{r summ_cars2}
summary(glm_post2)
```

## Generalized Linear Regression with binary outcome

We illustrate a logistic regression model using the example from a `rstanarm` vignette (https://cran.r-project.org/web/packages/rstanarm/vignettes/binomial.html)

> Gelman and Hill describe a survey of 3200 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells. The goal of the analysis presented by Gelman and Hill is to learn about the factors associated with switching wells.

```{r load_wells}
data(wells)
summary(wells)
# rescale dist to units of 100 meters
wells$dist100 <- wells$dist / 100
```


```{r fit_wells1, cache=TRUE, message=FALSE, results="hide"}
t_prior <- student_t(df = 7, location = 0, scale = 2.5)
glm_post3 <- stan_glm(switch ~ dist100, data = wells, 
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior)
```


```{r plt_wells1, fig.align="center"}
# Predicted probability as a function of x
pr_switch <- function(x, ests) plogis(ests[1] + ests[2] * x)

# A function to slightly jitter the binary data
jitt <- function(...) {
  geom_point(aes_string(...), position = position_jitter(height = 0.025, width = 0.1), 
             size = 2, shape = 21, stroke = 0.15)
}

ggplot(wells, aes(x = dist100, y = switch, color = switch)) + 
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  jitt(x="dist100") + 
  stat_function(fun = pr_switch, args = list(ests = coef(glm_post3)), 
                size = 1, color = "gray35")+
  theme(legend.position = "none")
```

```{r fit_wells2, cache=TRUE, message=FALSE, results="hide"}
glm_post4 <- update(glm_post3, formula = switch ~ dist100 + arsenic, refresh=0) 
```

```{r plt_wells2, fig.align="center"}
pr_switch2 <- function(x, y, ests) plogis(ests[1] + ests[2] * x + ests[3] * y)
grid <- expand.grid(dist100 = seq(0, 4, length.out = 100), 
                    arsenic = seq(0, 10, length.out = 100))
grid$prob <- with(grid, pr_switch2(dist100, arsenic, coef(glm_post4)))

ggplot(grid, aes(x = dist100, y = arsenic)) + 
  geom_tile(aes(fill = prob)) + 
  labs(fill = "Probability of \nSwitching") +
  ggtitle("Posterior Probability of switching by distance and arsenic level")
```

```{r fit_wells3, cache=TRUE, message=FALSE, results="hide"}
glm_post5 <- update(glm_post3, formula = switch ~ dist100*arsenic) 
```

```{r plt_wells3, fig.align="center"}
pr_switch3 <- function(x, y, ests) plogis(ests[1] + ests[2] * x + ests[3] * y + ests[4]*x*y)
grid$prob2 <- with(grid, pr_switch3(dist100, arsenic, coef(glm_post5)))

ggplot(grid, aes(x = dist100, y = arsenic)) + 
  geom_tile(aes(fill = prob2)) + 
  labs(fill = "Probability of \nSwitching") +
  ggtitle("Posterior Probability of switching by distance and arsenic level (w/ interaction)")
```

We can compare our three models using an approximation to Leave-One-Out (LOO) cross-validation, which is a method for estimating out of sample predictive performance and is implemented by the loo function in the loo package:

```{r comp_wells, cache=TRUE}
loo1 <- loo(glm_post3)
loo2 <- loo(glm_post4)
loo3 <- loo(glm_post5)
compare_models(loo1, loo2, loo3)
```

Including `arsenic` gives much better estimated predictive performance than `dist100` alone, but the interaction model is similar to the model with separate linear terms only.

```{r plt_wells_OR, fig.align="center"}
post_samp_OR<-exp( as.data.frame(glm_post4, pars=c("dist100","arsenic")) )

qplot(post_samp_OR[,"arsenic"],bins=40, xlab="posterior OR (arsenic)",ylab="")
mean(post_samp_OR[,"arsenic"] > 1.5)

qplot(post_samp_OR[,"dist100"],bins=40, xlab="posterior OR (distance)")
```


## Other rstanarm models

stan_betareg() - beta regression models
stan_biglm() - regularized linear but big models (data too large to fit in memery)
stan_clogit() - conditional logistic regression models
stan_gamm4() -  generalized linear additive models with optional group-specific terms
stan_glmer() - generalized linear models with group-specific terms
stan_lm() - regularized linear models
stan_mvmer() - multivariate generalized linear models with correlated group-specific terms
stan_nlmer() - nonlinear models with group-specific terms
stan_polr() - ordinal regression models
stan_jm() - joint longitudinal and time-to-event models

## Going beyond rstanarm

### Checking & modifying underlying Stan code

A good place to start is the first chapter of the Stan User's guide (https://mc-stan.org/users/documentation/).

The `rstan` code to fit the first linear regression model is below:

```{r stan_mod}
# define the stan model
stan_model <-
"
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
"

# format the data for stan
stan_data <- list(N=nrow(cars),x=cars$speed, y=cars$dist)
```

```{r stan_fit, cache=TRUE, results="hide"}
# sample from the model
glm_post_stan <- stan(model_code=stan_model, data=stan_data)
```

```{r stan_diag, fig.align="center"}
mcmc_trace(glm_post_stan)
```

```{r stan_summ}
summary(glm_post_stan)$summary
```

You can also get a sense for what is going on under the hood by looking at the stan model underlying any `rstanarm` model with the `get_stanmodel()` function.

```{r get_stanmodel}
# NOT run
# get_stanmodel(glm_post1$stanfit)
```

### Survival analysis

The current production version of `rstanarm` (v 2.18.2) does not have a function for survival analysis. There are ongoing discussions about implementing survival models in `rstanarm` at
https://discourse.mc-stan.org/t/survival-models-in-rstanarm/3998 and about the experimental function `stan_surv()`, currently on development branch of `rstanarm` at http://rstudio-pubs-static.s3.amazonaws.com/438966_3b8a25efb9b84454b8d69b7a15e3ebc5.html

A good alternative is `brms` another front-end to `rstan` that also uses formula syntax and can fit many of the same models.

```{r fit_surv, eval=FALSE}
library(brms)
# parametric survival analysis using the "weibull" family
surv_post <- brm(time | cens(censored) ~ age + sex + disease, 
            data = kidney, family = weibull, inits = "0")
```

## References

rstanarm page on the Stan site:
http://mc-stan.org/rstanarm/index.html

A lot of this material is borrowed from the official `rstanarm` vignettes by Jonah Gabry and Ben Goodrich:  
https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html  

https://cran.r-project.org/web/packages/rstanarm/vignettes/continuous.html

https://cran.rstudio.com/web/packages/rstanarm/vignettes/binomial.html

A good reference for Bayesian regression modeling is:
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. Chapman and Hall/CRC.
